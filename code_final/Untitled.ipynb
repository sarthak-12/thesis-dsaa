{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d88bf84-e76d-4d19-b159-f71167b156d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.6832706928253174, metrics={'train_runtime': 7.6976, 'train_samples_per_second': 0.779, 'train_steps_per_second': 0.39, 'total_flos': 0.0, 'train_loss': 0.6832706928253174, 'epoch': 3.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "# 1. Create a custom dataset class that returns tokenized text,\n",
    "#    numerical features, and labels.\n",
    "class FinancialNewsDataset(Dataset):\n",
    "    def __init__(self, texts, numerical_features, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.numerical_features = numerical_features\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        numerical = self.numerical_features[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Remove extra batch dimension\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding[\"numerical\"] = torch.tensor(numerical, dtype=torch.float)\n",
    "        encoding[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# 2. Define a custom multimodal model that fuses DeBERTa text representations\n",
    "#    with an MLP processing the numerical features.\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, text_model_name, num_numerical_features, num_labels):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "        # Load the pre-trained DeBERTa model\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        self.hidden_size = self.text_model.config.hidden_size\n",
    "        \n",
    "        # Define an MLP to process numerical features.\n",
    "        self.numerical_mlp = nn.Sequential(\n",
    "            nn.Linear(num_numerical_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Fusion and classification layers: concatenate text and numerical features.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size + 32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, numerical, labels=None):\n",
    "        # Process text through DeBERTa. Using the first token ([CLS] token) representation.\n",
    "        outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_repr = outputs.last_hidden_state[:, 0, :]  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Process the numerical features.\n",
    "        num_repr = self.numerical_mlp(numerical)  # (batch_size, 32)\n",
    "        \n",
    "        # Concatenate the representations.\n",
    "        combined = torch.cat((text_repr, num_repr), dim=1)\n",
    "        \n",
    "        # Final classification.\n",
    "        logits = self.classifier(combined)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# 3. Prepare the tokenizer, dummy data, and dataset.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "# Dummy data â€“ replace these with your actual news articles, numerical features, and labels.\n",
    "texts = [\n",
    "    \"Financial markets rally as economic indicators improve.\",\n",
    "    \"Stock prices decline amid economic uncertainty.\"\n",
    "]\n",
    "# Example numerical features (e.g., technical indicators); adjust dimension as needed.\n",
    "numerical_features = [\n",
    "    [0.5, 0.3, 0.1],\n",
    "    [0.2, 0.4, 0.6]\n",
    "]\n",
    "# Binary labels: 1 for up, 0 for down.\n",
    "labels = [1, 0]\n",
    "\n",
    "# Create the dataset.\n",
    "dataset = FinancialNewsDataset(texts, numerical_features, labels, tokenizer, max_length=128)\n",
    "\n",
    "# 4. Instantiate the multimodal model.\n",
    "num_numerical_features = 3  # Set according to your numerical data\n",
    "num_labels = 2              # For binary classification\n",
    "model = MultimodalClassifier(\"microsoft/deberta-v3-base\", num_numerical_features, num_labels)\n",
    "\n",
    "# 5. Define training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Data collator to handle dynamic padding.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 6. Instantiate the Trainer.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,  # Replace with your train dataset\n",
    "    eval_dataset=dataset,   # Replace with your validation dataset\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 7. Fine-tune the model.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed55df-1945-4eab-a46b-b5b5264eda70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
